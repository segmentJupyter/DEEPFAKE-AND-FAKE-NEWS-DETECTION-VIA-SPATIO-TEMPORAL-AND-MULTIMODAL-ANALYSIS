{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c58a00-8f71-4a90-b5f5-3eb3a05c7097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking and installing dependencies ===\n",
      "\n",
      "=== Using device: cpu ===\n",
      "\n",
      "======================================================================\n",
      "ADVANCED TCN DEEPFAKE DETECTION\n",
      "Target: 90%+ Accuracy\n",
      "======================================================================\n",
      "\n",
      "=== Creating training dataset (with augmentation) ===\n",
      "\n",
      "=== Loading dataset ===\n",
      "Loaded 100 real videos\n",
      "Loaded 100 fake videos\n",
      "Total dataset size: 200 videos\n",
      "\n",
      "=== Creating validation/test dataset (no augmentation) ===\n",
      "\n",
      "=== Loading dataset ===\n",
      "Loaded 100 real videos\n",
      "Loaded 100 fake videos\n",
      "Total dataset size: 200 videos\n",
      "\n",
      "Dataset split: Train=140, Val=30, Test=30\n",
      "\n",
      "======================================================================\n",
      "TRAINING ADVANCED TCN MODEL\n",
      "======================================================================\n",
      "\n",
      "Model Parameters:\n",
      "  Total: 12,065,475\n",
      "  Trainable: 12,065,475\n",
      "\n",
      "======================================================================\n",
      "TRAINING ADVANCED TCN MODEL\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/18 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [36:12<00:00, 62.06s/it, loss=0.0576]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0535, Train Acc: 0.5357 | Val Loss: 0.0429, Val Acc: 0.6000 | LR: 0.0000488\n",
      "‚úì Saved best model with Val Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/18 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [30:21<00:00, 52.05s/it, loss=0.0715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0510, Train Acc: 0.5143 | Val Loss: 0.0427, Val Acc: 0.6000 | LR: 0.0000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/18 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [30:27<00:00, 52.21s/it, loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0555, Train Acc: 0.4143 | Val Loss: 0.0407, Val Acc: 0.6000 | LR: 0.0000397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/18 [Train]:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 18/35 [16:39<15:43, 55.51s/it, loss=0.0370]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 754\u001b[0m\n\u001b[0;32m    750\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 754\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 720\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Trainable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 720\u001b[0m tcn_history \u001b[38;5;241m=\u001b[39m train_model(tcn_model, train_loader, val_loader, EPOCHS)\n\u001b[0;32m    721\u001b[0m tcn_results \u001b[38;5;241m=\u001b[39m evaluate_model(tcn_model, test_loader)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 452\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m    449\u001b[0m train_preds, train_labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    451\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (frames, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m         frames, labels \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda_dibs\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda_dibs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda_dibs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda_dibs\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda_dibs\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[1], line 122\u001b[0m, in \u001b[0;36mAdvancedVideoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    119\u001b[0m video_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_frames(video_path)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment:\n\u001b[0;32m    124\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_frames(frames)\n",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m, in \u001b[0;36mAdvancedVideoDataset.extract_frames\u001b[1;34m(self, video_path)\u001b[0m\n\u001b[0;32m    149\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, total_frames\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m--> 152\u001b[0m     cap\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_FRAMES, idx)\n\u001b[0;32m    153\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;66;03m# Resize\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced TCN Deepfake Detection - Optimized for 90%+ Accuracy\n",
    "Enhanced temporal modeling with attention mechanisms and advanced training\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install(package):\n",
    "    \"\"\"Auto-install missing packages\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"‚úì Installed {package}\")\n",
    "    except:\n",
    "        print(f\"! Warning: Could not install {package}, continuing...\")\n",
    "\n",
    "# Auto-install required packages\n",
    "required_packages = {\n",
    "    'torch': 'torch torchvision',\n",
    "    'cv2': 'opencv-python',\n",
    "    'numpy': 'numpy',\n",
    "    'sklearn': 'scikit-learn',\n",
    "    'PIL': 'Pillow',\n",
    "    'tqdm': 'tqdm',\n",
    "    'matplotlib': 'matplotlib'\n",
    "}\n",
    "\n",
    "print(\"=== Checking and installing dependencies ===\")\n",
    "for module, package in required_packages.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install(package)\n",
    "\n",
    "# Now import everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n=== Using device: {DEVICE} ===\")\n",
    "\n",
    "# Updated Paths\n",
    "DATA_PATHS = {\n",
    "    'real': r\"C:\\Users\\dibya\\OneDrive\\Desktop\\FF++\\real\",\n",
    "    'fake': r\"C:\\Users\\dibya\\OneDrive\\Desktop\\FF++\\fake\"\n",
    "}\n",
    "\n",
    "# Optimized Hyperparameters for High Accuracy\n",
    "IMG_SIZE = 224  # Increased for better feature extraction\n",
    "SEQUENCE_LENGTH = 24  # Longer sequences for better temporal patterns\n",
    "BATCH_SIZE = 4  # Smaller batch for more stable gradients\n",
    "EPOCHS = 18  # Reduced epochs for faster testing\n",
    "LEARNING_RATE = 0.00005  # Lower learning rate for fine-grained optimization\n",
    "WEIGHT_DECAY = 1e-5  # L2 regularization\n",
    "MAX_VIDEOS_PER_CLASS = 100  # More data for better generalization\n",
    "\n",
    "\n",
    "class AdvancedVideoDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with advanced augmentation techniques\"\"\"\n",
    "    def __init__(self, data_paths, sequence_length=24, img_size=224, max_videos=100, augment=False):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.data = []\n",
    "        \n",
    "        print(\"\\n=== Loading dataset ===\")\n",
    "        \n",
    "        # Load real videos (label = 0)\n",
    "        real_path = Path(data_paths['real'])\n",
    "        if real_path.exists():\n",
    "            real_videos = list(real_path.glob('**/*.mp4'))[:max_videos]\n",
    "            self.data.extend([(str(v), 0) for v in real_videos])\n",
    "            print(f\"Loaded {len(real_videos)} real videos\")\n",
    "        else:\n",
    "            print(f\"! Warning: Real video path not found: {real_path}\")\n",
    "        \n",
    "        # Load fake videos (label = 1)\n",
    "        fake_path = Path(data_paths['fake'])\n",
    "        if fake_path.exists():\n",
    "            fake_videos = list(fake_path.glob('**/*.mp4'))[:max_videos]\n",
    "            self.data.extend([(str(v), 1) for v in fake_videos])\n",
    "            print(f\"Loaded {len(fake_videos)} fake videos\")\n",
    "        else:\n",
    "            print(f\"! Warning: Fake video path not found: {fake_path}\")\n",
    "        \n",
    "        print(f\"Total dataset size: {len(self.data)} videos\")\n",
    "        \n",
    "        # Shuffle data\n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(\"No videos found! Check your paths.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            frames = self.extract_frames(video_path)\n",
    "            if self.augment:\n",
    "                frames = self.augment_frames(frames)\n",
    "            return frames, label\n",
    "        except Exception as e:\n",
    "            print(f\"! Error loading {video_path}: {e}\")\n",
    "            frames = torch.zeros(self.sequence_length, 3, self.img_size, self.img_size, dtype=torch.float32)  # Ensure float32\n",
    "            return frames, label\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract frames with advanced preprocessing\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            raise ValueError(f\"No frames in video: {video_path}\")\n",
    "        \n",
    "        # Sample frames evenly\n",
    "        if total_frames < self.sequence_length:\n",
    "            indices = list(range(total_frames))\n",
    "            while len(indices) < self.sequence_length:\n",
    "                indices.extend(list(range(total_frames)))\n",
    "            indices = indices[:self.sequence_length]\n",
    "        else:\n",
    "            indices = np.linspace(0, total_frames-1, self.sequence_length, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                # Resize\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Advanced normalization\n",
    "                frame = frame.astype(np.float32) / 255.0\n",
    "                \n",
    "                # ImageNet normalization for better feature extraction\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                frame = (frame - mean) / std\n",
    "                \n",
    "                frame = torch.from_numpy(frame).permute(2, 0, 1).float()  # Ensure float32\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                if frames:\n",
    "                    frames.append(frames[-1].clone())\n",
    "                else:\n",
    "                    frames.append(torch.zeros(3, self.img_size, self.img_size, dtype=torch.float32))  # Ensure float32\n",
    "        \n",
    "        cap.release()\n",
    "        return torch.stack(frames)\n",
    "    \n",
    "    def augment_frames(self, frames):\n",
    "        \"\"\"Advanced data augmentation\"\"\"\n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            frames = torch.flip(frames, dims=[3])\n",
    "        \n",
    "        # Random brightness adjustment\n",
    "        if random.random() > 0.5:\n",
    "            brightness_factor = random.uniform(0.7, 1.3)\n",
    "            frames = frames * brightness_factor\n",
    "        \n",
    "        # Random contrast adjustment\n",
    "        if random.random() > 0.5:\n",
    "            contrast_factor = random.uniform(0.8, 1.2)\n",
    "            mean = frames.mean(dim=[2, 3], keepdim=True)\n",
    "            frames = (frames - mean) * contrast_factor + mean\n",
    "        \n",
    "        # Random noise injection\n",
    "        if random.random() > 0.7:\n",
    "            noise = torch.randn_like(frames) * 0.02\n",
    "            frames = frames + noise\n",
    "        \n",
    "        # Random rotation (small angles)\n",
    "        if random.random() > 0.7:\n",
    "            angle = random.uniform(-5, 5)\n",
    "            # Simple rotation by shifting\n",
    "            shift = int(angle / 5)\n",
    "            if shift != 0:\n",
    "                frames = torch.roll(frames, shifts=shift, dims=3)\n",
    "        \n",
    "        return frames\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism for temporal features\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.query = nn.Conv1d(channels, channels // 8, 1)\n",
    "        self.key = nn.Conv1d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv1d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, channels, time]\n",
    "        batch_size, channels, time = x.size()\n",
    "        \n",
    "        # Compute attention\n",
    "        proj_query = self.query(x).permute(0, 2, 1)  # [B, T, C//8]\n",
    "        proj_key = self.key(x)  # [B, C//8, T]\n",
    "        \n",
    "        energy = torch.bmm(proj_query, proj_key)  # [B, T, T]\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        \n",
    "        proj_value = self.value(x)  # [B, C, T]\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        \n",
    "        # Residual connection with learnable weight\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class AdvancedTemporalBlock(nn.Module):\n",
    "    \"\"\"Enhanced Temporal Convolutional Block with SE attention\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(AdvancedTemporalBlock, self).__init__()\n",
    "        \n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              padding=self.padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                              padding=self.padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Squeeze-and-Excitation block\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(out_channels, out_channels // 4, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels // 4, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Residual connection\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv\n",
    "        out = self.conv1(x)\n",
    "        if self.padding > 0:\n",
    "            out = out[:, :, :-self.padding]\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        # Second conv\n",
    "        out = self.conv2(out)\n",
    "        if self.padding > 0:\n",
    "            out = out[:, :, :-self.padding]\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # SE attention\n",
    "        se_weight = self.se(out)\n",
    "        out = out * se_weight\n",
    "        \n",
    "        # Residual connection\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        \n",
    "        # Match dimensions\n",
    "        if out.size(2) != res.size(2):\n",
    "            res = res[:, :, :out.size(2)]\n",
    "        \n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class AdvancedTCN(nn.Module):\n",
    "    \"\"\"State-of-the-art TCN model with attention and advanced architectures\"\"\"\n",
    "    def __init__(self):\n",
    "        super(AdvancedTCN, self).__init__()\n",
    "        \n",
    "        # Enhanced spatial feature extractor (ResNet-like)\n",
    "        self.spatial = nn.Sequential(\n",
    "            # Initial conv\n",
    "            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            \n",
    "            # Block 1\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Advanced temporal processing\n",
    "        self.temporal = nn.Sequential(\n",
    "            AdvancedTemporalBlock(512, 512, kernel_size=3, dilation=1),\n",
    "            AdvancedTemporalBlock(512, 512, kernel_size=3, dilation=2),\n",
    "            AdvancedTemporalBlock(512, 512, kernel_size=3, dilation=4),\n",
    "            AdvancedTemporalBlock(512, 512, kernel_size=3, dilation=8),\n",
    "        )\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.attention = TemporalAttention(512)\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, channels, height, width]\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        \n",
    "        # Extract spatial features\n",
    "        x = x.view(batch_size * seq_len, *x.shape[2:])\n",
    "        x = self.spatial(x)\n",
    "        x = x.view(batch_size, 512, seq_len)\n",
    "        \n",
    "        # Extract temporal features\n",
    "        x = self.temporal(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Multi-scale temporal pooling\n",
    "        x_max = torch.max(x, dim=2)[0]\n",
    "        x_avg = torch.mean(x, dim=2)\n",
    "        x_std = torch.std(x, dim=2)\n",
    "        \n",
    "        # Combine different pooling strategies\n",
    "        x = x_max + x_avg + 0.5 * x_std\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=18):\n",
    "    \"\"\"Advanced training with mixed precision and gradient accumulation\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING ADVANCED TCN MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Use Focal Loss for better handling of hard examples\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    \n",
    "    # AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Cosine annealing with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Enable mixed precision training if available\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for batch_idx, (frames, labels) in enumerate(pbar):\n",
    "            try:\n",
    "                frames, labels = frames.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Mixed precision forward pass\n",
    "                if scaler is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(frames)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(frames)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            except Exception as e:\n",
    "                print(f\"! Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for frames, labels in val_loader:\n",
    "                try:\n",
    "                    frames, labels = frames.to(DEVICE), labels.to(DEVICE)\n",
    "                    \n",
    "                    if scaler is not None:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(frames)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = model(frames)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"! Validation error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | LR: {current_lr:.7f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'TCN_best.pth')\n",
    "            print(f\"‚úì Saved best model with Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n‚úì Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('TCN_best.pth'))\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Comprehensive evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Evaluating TCN\"):\n",
    "            try:\n",
    "                frames = frames.to(DEVICE)\n",
    "                outputs = model(frames)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ADVANCED TCN RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"              Real  Fake\")\n",
    "    print(f\"Actual Real   {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "    print(f\"       Fake   {cm[1][0]:4d}  {cm[1][1]:4d}\")\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
    "\n",
    "\n",
    "def plot_results(history, results):\n",
    "    \"\"\"Visualization of training and results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', marker='o', linewidth=2, color='#2196F3')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation', marker='s', linewidth=2, color='#FF9800')\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontsize=14, weight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train', marker='o', linewidth=2, color='#4CAF50')\n",
    "    axes[0, 1].plot(history['val_acc'], label='Validation', marker='s', linewidth=2, color='#F44336')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, weight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='90% Target')\n",
    "    \n",
    "    # Final Metrics\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    values = [results[m] for m in metrics]\n",
    "    \n",
    "    colors = ['#4CAF50' if v >= 0.9 else '#FF9800' if v >= 0.8 else '#F44336' for v in values]\n",
    "    bars = axes[1, 0].bar(range(len(metrics)), values, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('Final Performance Metrics', fontsize=14, weight='bold')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_xticks(range(len(metrics)))\n",
    "    axes[1, 0].set_xticklabels([m.upper() for m in metrics])\n",
    "    axes[1, 0].grid(True, axis='y', alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    axes[1, 0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{height:.3f}', ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "    \n",
    "    # Performance Summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "         ADVANCED TCN PERFORMANCE\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    üìä Test Set Results:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Accuracy:     {results['accuracy']*100:6.2f}%\n",
    "    Precision:    {results['precision']*100:6.2f}%\n",
    "    Recall:       {results['recall']*100:6.2f}%\n",
    "    F1-Score:     {results['f1']*100:6.2f}%\n",
    "    AUC-ROC:      {results['auc']*100:6.2f}%\n",
    "    \n",
    "    üéØ Target Achievement:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    90% Accuracy: {'‚úì ACHIEVED' if results['accuracy'] >= 0.9 else '‚úó Not Yet'}\n",
    "    \n",
    "    üìà Training Performance:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Best Val Acc: {max(history['val_acc'])*100:6.2f}%\n",
    "    Final Loss:   {history['val_loss'][-1]:6.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.5, 0.5, summary_text, ha='center', va='center',\n",
    "                   fontsize=11, family='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='#E8F5E9', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_tcn_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n‚úì Results saved to 'advanced_tcn_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ADVANCED TCN DEEPFAKE DETECTION\")\n",
    "        print(\"Target: 90%+ Accuracy\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load dataset with augmentation for training\n",
    "        print(\"\\n=== Creating training dataset (with augmentation) ===\")\n",
    "        train_dataset_full = AdvancedVideoDataset(\n",
    "            DATA_PATHS, SEQUENCE_LENGTH, IMG_SIZE, MAX_VIDEOS_PER_CLASS, augment=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Creating validation/test dataset (no augmentation) ===\")\n",
    "        eval_dataset_full = AdvancedVideoDataset(\n",
    "            DATA_PATHS, SEQUENCE_LENGTH, IMG_SIZE, MAX_VIDEOS_PER_CLASS, augment=False\n",
    "        )\n",
    "        \n",
    "        # Split dataset\n",
    "        total_size = len(train_dataset_full)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.15 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        train_indices = list(range(train_size))\n",
    "        val_indices = list(range(train_size, train_size + val_size))\n",
    "        test_indices = list(range(train_size + val_size, total_size))\n",
    "        \n",
    "        train_dataset = torch.utils.data.Subset(train_dataset_full, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(eval_dataset_full, val_indices)\n",
    "        test_dataset = torch.utils.data.Subset(eval_dataset_full, test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                 num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                               num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                num_workers=0, pin_memory=True)\n",
    "        \n",
    "        print(f\"\\nDataset split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "        \n",
    "        # Train TCN\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TRAINING ADVANCED TCN MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        tcn_model = AdvancedTCN().to(DEVICE)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in tcn_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in tcn_model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nModel Parameters:\")\n",
    "        print(f\"  Total: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "        \n",
    "        tcn_history = train_model(tcn_model, train_loader, val_loader, EPOCHS)\n",
    "        tcn_results = evaluate_model(tcn_model, test_loader)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_results(tcn_history, tcn_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìä FINAL TCN RESULTS:\")\n",
    "        print(f\"  Accuracy:  {tcn_results['accuracy']:.4f} ({tcn_results['accuracy']*100:.2f}%)\")\n",
    "        print(f\"  Precision: {tcn_results['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {tcn_results['recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {tcn_results['f1']:.4f}\")\n",
    "        print(f\"  AUC-ROC:   {tcn_results['auc']:.4f}\")\n",
    "        \n",
    "        if tcn_results['accuracy'] >= 0.9:\n",
    "            print(f\"\\nüéâ TARGET ACHIEVED! Accuracy >= 90%\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Target not reached. Consider:\")\n",
    "            print(f\"     - Increasing MAX_VIDEOS_PER_CLASS\")\n",
    "            print(f\"     - Training for more epochs\")\n",
    "            print(f\"     - Adjusting learning rate\")\n",
    "        \n",
    "        print(f\"\\n‚úì Best model saved as 'TCN_best.pth'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n! CRITICAL ERROR: {e}\")\n",
    "        print(\"! Check your data paths and ensure videos exist.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefa282-6c4a-409c-9281-1afa71153f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
