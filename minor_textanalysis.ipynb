{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f1aa74-14df-4efa-a397-49cb01e5ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Checking dependencies...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ ADVANCED EDA - MULTIMODAL FAKE NEWS DETECTION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "[PHASE 1] ADVANCED TEXT DATA ANALYSIS\n",
      "================================================================================\n",
      "âœ“ Loaded 44,898 articles (23,481 fake, 21,417 true)\n",
      "  Columns: ['title', 'text', 'subject', 'date', 'label', 'category']\n",
      "\n",
      "ðŸ“ Text Length Analysis:\n",
      "\n",
      "Fake News Statistics:\n",
      "  Avg length: 2547 chars\n",
      "  Avg words: 423\n",
      "\n",
      "True News Statistics:\n",
      "  Avg length: 2383 chars\n",
      "  Avg words: 386\n",
      "âœ“ Saved: eda_outputs/text_length_analysis.png\n",
      "\n",
      "ðŸ“° Title Analysis:\n",
      "\n",
      "Fake News Titles:\n",
      "  Avg length: 94.2 chars\n",
      "  Questions: 7.0%\n",
      "  Exclamations: 13.9%\n",
      "\n",
      "True News Titles:\n",
      "  Avg length: 64.7 chars\n",
      "  Questions: 0.6%\n",
      "  Exclamations: 0.1%\n",
      "âœ“ Saved: eda_outputs/title_analysis.png\n",
      "\n",
      "â˜ï¸ Generating Word Clouds...\n",
      "âœ“ Saved: eda_outputs/wordclouds.png\n",
      "\n",
      "ðŸ“Š N-Gram Analysis...\n",
      "\n",
      "Top 10 Bigrams (Fake News):\n",
      "  donald trump: 16,288\n",
      "  featured image: 7,692\n",
      "  hillary clinton: 7,278\n",
      "  white house: 6,743\n",
      "  united states: 6,663\n",
      "  twitter com: 6,567\n",
      "  pic twitter: 6,195\n",
      "  new york: 4,352\n",
      "  president obama: 4,081\n",
      "  getty images: 4,022\n",
      "\n",
      "Top 10 Bigrams (True News):\n",
      "  united states: 12,204\n",
      "  donald trump: 10,168\n",
      "  white house: 8,419\n",
      "  washington reuters: 6,674\n",
      "  president donald: 5,930\n",
      "  north korea: 5,659\n",
      "  new york: 4,740\n",
      "  prime minister: 4,163\n",
      "  said statement: 3,933\n",
      "  trump said: 3,551\n",
      "âœ“ Saved: eda_outputs/bigrams_analysis.png\n",
      "\n",
      "ðŸ” TF-IDF Analysis...\n",
      "  TF-IDF matrix shape: (44898, 1000)\n",
      "  Vocabulary size: 1000\n",
      "âœ“ Saved: eda_outputs/tfidf_pca.png\n",
      "\n",
      "================================================================================\n",
      "[PHASE 2] ADVANCED IMAGE DATASET ANALYSIS\n",
      "================================================================================\n",
      "âœ“ Found 8210 Reddit images\n",
      "\n",
      "ðŸ“¸ Detailed Image Analysis...\n",
      "  Processing 150/200...\n",
      "\n",
      "ðŸ“Š Image Statistics (n=200):\n",
      "  Dimensions: 307x292 (avg)\n",
      "  Aspect ratio: 1.29 Â± 0.69\n",
      "  File size: 16.8 Â± 9.4 KB\n",
      "  Brightness: 111.0 Â± 44.2\n",
      "  Contrast: 56.1 Â± 15.7\n",
      "âœ“ Saved: eda_outputs/image_properties.png\n",
      "âœ“ Saved: eda_outputs/image_formats.png\n",
      "\n",
      "================================================================================\n",
      "[PHASE 3] DEEP CNN FEATURE EXTRACTION & ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ§  Loading pre-trained models...\n",
      "ðŸ” Extracting features from 100 images...\n",
      "  Progress: 80/100\n",
      "âœ“ Extracted features shape: (100, 100352)\n",
      "\n",
      "ðŸ“ˆ Deep Feature Statistics:\n",
      "  Mean activation: 0.4831\n",
      "  Std activation: 1.7199\n",
      "  Max activation: 71.6312\n",
      "  Min activation: 0.0000\n",
      "  Sparsity: 82.30%\n",
      "âœ“ Saved: eda_outputs/cnn_feature_distribution.png\n",
      "\n",
      "ðŸ“Š Creating t-SNE visualization...\n",
      "âš  t-SNE visualization failed: 'NoneType' object has no attribute 'split'\n",
      "âš  UMAP not available (install: pip install umap-learn)\n",
      "\n",
      "ðŸ“Š PCA Analysis...\n",
      "  Components for 95% variance: 89\n",
      "  Components for 99% variance: 96\n",
      "âœ“ Saved: eda_outputs/pca_analysis.png\n",
      "\n",
      "================================================================================\n",
      "[PHASE 4] CORRELATION & STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Computing correlations...\n",
      "âœ“ Saved: eda_outputs/correlation_matrix.png\n",
      "\n",
      "ðŸ”¬ Statistical Tests (Fake vs True):\n",
      "\n",
      "  Text Length T-Test:\n",
      "    t-statistic: 8.0039\n",
      "    p-value: 0.000000\n",
      "    Significant: Yes\n",
      "\n",
      "  Word Count Mann-Whitney U Test:\n",
      "    U-statistic: 261434956.5000\n",
      "    p-value: 0.000000\n",
      "    Significant: Yes\n",
      "\n",
      "  Exclamation vs Label Chi-Square Test:\n",
      "    chi2-statistic: 3162.1921\n",
      "    p-value: 0.000000\n",
      "    Significant: Yes\n",
      "\n",
      "================================================================================\n",
      "[PHASE 5] GENERATING COMPREHENSIVE SUMMARY\n",
      "================================================================================\n",
      "âœ“ Saved: eda_outputs/summary_dashboard.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ADVANCED EDA COMPLETED\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Generated Outputs (in eda_outputs/):\n",
      "  âœ“ text_length_analysis.png\n",
      "  âœ“ title_analysis.png\n",
      "  âœ“ wordclouds.png\n",
      "  âœ“ bigrams_analysis.png\n",
      "  âœ“ tfidf_pca.png\n",
      "  âœ“ image_properties.png\n",
      "  âœ“ image_formats.png\n",
      "  âœ“ cnn_feature_distribution.png\n",
      "  âœ“ pca_analysis.png\n",
      "  âœ“ correlation_matrix.png\n",
      "  âœ“ summary_dashboard.png\n",
      "\n",
      "ðŸ”¬ Analysis Components Completed:\n",
      "  âœ“ Text length & word count analysis\n",
      "  âœ“ Title characteristics (punctuation, caps)\n",
      "  âœ“ Word clouds (fake vs true)\n",
      "  âœ“ N-gram analysis (bigrams)\n",
      "  âœ“ TF-IDF feature extraction & PCA\n",
      "  âœ“ Image properties (dimensions, brightness, contrast)\n",
      "  âœ“ Deep CNN feature extraction (ResNet50)\n",
      "  âœ“ t-SNE & UMAP visualizations\n",
      "  âœ“ PCA dimensionality analysis\n",
      "  âœ“ Statistical hypothesis testing\n",
      "  âœ“ Correlation analysis\n",
      "  âœ“ Comprehensive summary dashboard\n",
      "\n",
      "ðŸ“Š Key Insights:\n",
      "  â€¢ Dataset balance: 23,481 fake vs 21,417 true articles\n",
      "  â€¢ Text length difference: 6.9%\n",
      "  â€¢ Exclamation usage: 13.9% (fake) vs 0.1% (true)\n",
      "\n",
      "ðŸŽ¯ Next Steps:\n",
      "  1. Feature engineering based on EDA insights\n",
      "  2. Model selection (CNN for images, LSTM/Transformer for text)\n",
      "  3. Multimodal fusion architecture design\n",
      "  4. Training with proper validation strategy\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install(package):\n",
    "    \"\"\"Auto-install missing packages\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"âœ“ Installed {package}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Required packages\n",
    "packages = {\n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy', \n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'pillow': 'PIL',\n",
    "    'scikit-learn': 'sklearn',\n",
    "    'tensorflow': 'tensorflow',\n",
    "    'opencv-python': 'cv2',\n",
    "    'wordcloud': 'wordcloud',\n",
    "    'nltk': 'nltk'\n",
    "}\n",
    "\n",
    "print(\"ðŸ”§ Checking dependencies...\")\n",
    "for name, import_name in packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {name}...\")\n",
    "        install(name)\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "import glob\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# NLP imports\n",
    "try:\n",
    "    import nltk\n",
    "    from wordcloud import WordCloud\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    NLP_AVAILABLE = True\n",
    "except:\n",
    "    NLP_AVAILABLE = False\n",
    "    print(\"âš  NLP libraries limited\")\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    DL_AVAILABLE = True\n",
    "except:\n",
    "    DL_AVAILABLE = False\n",
    "    print(\"âš  Deep learning limited\")\n",
    "\n",
    "# OpenCV\n",
    "try:\n",
    "    import cv2\n",
    "    CV_AVAILABLE = True\n",
    "except:\n",
    "    CV_AVAILABLE = False\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================\n",
    "FAKE_PATH = r\"C:\\Users\\dibya\\OneDrive\\Desktop\\miniproj\\Fake.csv\"\n",
    "TRUE_PATH = r\"C:\\Users\\dibya\\OneDrive\\Desktop\\miniproj\\True.csv\"\n",
    "REDDIT_PATH = r\"C:\\Users\\dibya\\OneDrive\\Desktop\\miniproj\\fakeddit_subset\"\n",
    "MEDICAL_PATH = r\"C:\\Users\\dibya\\OneDrive\\Desktop\\miniproj\\snopes_medical\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('eda_outputs', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”¬ ADVANCED EDA - MULTIMODAL FAKE NEWS DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =====================================================\n",
    "# PHASE 1: ADVANCED TEXT ANALYSIS\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PHASE 1] ADVANCED TEXT DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_text = None\n",
    "df_fake = None\n",
    "df_true = None\n",
    "\n",
    "try:\n",
    "    if os.path.exists(FAKE_PATH) and os.path.exists(TRUE_PATH):\n",
    "        df_fake = pd.read_csv(FAKE_PATH)\n",
    "        df_fake['label'] = 0\n",
    "        df_fake['category'] = 'Fake'\n",
    "        \n",
    "        df_true = pd.read_csv(TRUE_PATH)\n",
    "        df_true['label'] = 1\n",
    "        df_true['category'] = 'True'\n",
    "        \n",
    "        df_text = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "        print(f\"âœ“ Loaded {len(df_text):,} articles ({len(df_fake):,} fake, {len(df_true):,} true)\")\n",
    "        print(f\"  Columns: {df_text.columns.tolist()}\")\n",
    "        \n",
    "        # ===== 1.1 TEXT LENGTH ANALYSIS =====\n",
    "        print(\"\\nðŸ“ Text Length Analysis:\")\n",
    "        if 'text' in df_text.columns:\n",
    "            df_text['text_length'] = df_text['text'].astype(str).apply(len)\n",
    "            df_text['word_count'] = df_text['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "            \n",
    "            # Add to individual dataframes too\n",
    "            df_fake['text_length'] = df_fake['text'].astype(str).apply(len)\n",
    "            df_fake['word_count'] = df_fake['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "            df_true['text_length'] = df_true['text'].astype(str).apply(len)\n",
    "            df_true['word_count'] = df_true['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "            \n",
    "            print(f\"\\nFake News Statistics:\")\n",
    "            print(f\"  Avg length: {df_fake['text_length'].mean():.0f} chars\")\n",
    "            print(f\"  Avg words: {df_fake['word_count'].mean():.0f}\")\n",
    "            \n",
    "            print(f\"\\nTrue News Statistics:\")\n",
    "            print(f\"  Avg length: {df_true['text_length'].mean():.0f} chars\")\n",
    "            print(f\"  Avg words: {df_true['word_count'].mean():.0f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            \n",
    "            # Text length distribution\n",
    "            axes[0, 0].hist(df_fake['text_length'], bins=50, alpha=0.6, label='Fake', color='red', edgecolor='black')\n",
    "            axes[0, 0].hist(df_true['text_length'], bins=50, alpha=0.6, label='True', color='green', edgecolor='black')\n",
    "            axes[0, 0].set_xlabel('Text Length (characters)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].set_title('Text Length Distribution', fontweight='bold', fontsize=12)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Word count distribution\n",
    "            axes[0, 1].hist(df_fake['word_count'], bins=50, alpha=0.6, label='Fake', color='red', edgecolor='black')\n",
    "            axes[0, 1].hist(df_true['word_count'], bins=50, alpha=0.6, label='True', color='green', edgecolor='black')\n",
    "            axes[0, 1].set_xlabel('Word Count')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].set_title('Word Count Distribution', fontweight='bold', fontsize=12)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # Box plots\n",
    "            data_to_plot = [df_fake['text_length'], df_true['text_length']]\n",
    "            axes[1, 0].boxplot(data_to_plot, labels=['Fake', 'True'], patch_artist=True,\n",
    "                              boxprops=dict(facecolor='lightblue'))\n",
    "            axes[1, 0].set_ylabel('Text Length (characters)')\n",
    "            axes[1, 0].set_title('Text Length Comparison', fontweight='bold', fontsize=12)\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "            \n",
    "            data_to_plot = [df_fake['word_count'], df_true['word_count']]\n",
    "            axes[1, 1].boxplot(data_to_plot, labels=['Fake', 'True'], patch_artist=True,\n",
    "                              boxprops=dict(facecolor='lightcoral'))\n",
    "            axes[1, 1].set_ylabel('Word Count')\n",
    "            axes[1, 1].set_title('Word Count Comparison', fontweight='bold', fontsize=12)\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/text_length_analysis.png\")\n",
    "        \n",
    "        # ===== 1.2 TITLE ANALYSIS =====\n",
    "        if 'title' in df_text.columns:\n",
    "            print(\"\\nðŸ“° Title Analysis:\")\n",
    "            df_text['title_length'] = df_text['title'].astype(str).apply(len)\n",
    "            df_text['title_word_count'] = df_text['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "            df_text['has_question'] = df_text['title'].astype(str).str.contains('\\?', regex=True)\n",
    "            df_text['has_exclamation'] = df_text['title'].astype(str).str.contains('!', regex=False)\n",
    "            df_text['all_caps_words'] = df_text['title'].astype(str).apply(lambda x: sum(1 for word in x.split() if word.isupper() and len(word) > 2))\n",
    "            \n",
    "            # Add to individual dataframes too\n",
    "            df_fake['title_length'] = df_fake['title'].astype(str).apply(len)\n",
    "            df_fake['title_word_count'] = df_fake['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "            df_fake['has_question'] = df_fake['title'].astype(str).str.contains('\\?', regex=True)\n",
    "            df_fake['has_exclamation'] = df_fake['title'].astype(str).str.contains('!', regex=False)\n",
    "            df_fake['all_caps_words'] = df_fake['title'].astype(str).apply(lambda x: sum(1 for word in x.split() if word.isupper() and len(word) > 2))\n",
    "            \n",
    "            df_true['title_length'] = df_true['title'].astype(str).apply(len)\n",
    "            df_true['title_word_count'] = df_true['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "            df_true['has_question'] = df_true['title'].astype(str).str.contains('\\?', regex=True)\n",
    "            df_true['has_exclamation'] = df_true['title'].astype(str).str.contains('!', regex=False)\n",
    "            df_true['all_caps_words'] = df_true['title'].astype(str).apply(lambda x: sum(1 for word in x.split() if word.isupper() and len(word) > 2))\n",
    "            \n",
    "            print(f\"\\nFake News Titles:\")\n",
    "            print(f\"  Avg length: {df_fake['title_length'].mean():.1f} chars\")\n",
    "            print(f\"  Questions: {(df_fake['has_question'].sum() / len(df_fake) * 100):.1f}%\")\n",
    "            print(f\"  Exclamations: {(df_fake['has_exclamation'].sum() / len(df_fake) * 100):.1f}%\")\n",
    "            \n",
    "            print(f\"\\nTrue News Titles:\")\n",
    "            print(f\"  Avg length: {df_true['title_length'].mean():.1f} chars\")\n",
    "            print(f\"  Questions: {(df_true['has_question'].sum() / len(df_true) * 100):.1f}%\")\n",
    "            print(f\"  Exclamations: {(df_true['has_exclamation'].sum() / len(df_true) * 100):.1f}%\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "            \n",
    "            # Title length\n",
    "            axes[0, 0].hist(df_fake['title_length'], bins=30, alpha=0.6, label='Fake', color='red')\n",
    "            axes[0, 0].hist(df_true['title_length'], bins=30, alpha=0.6, label='True', color='green')\n",
    "            axes[0, 0].set_xlabel('Title Length')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].set_title('Title Length Distribution', fontweight='bold')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Punctuation usage\n",
    "            punct_data = pd.DataFrame({\n",
    "                'Category': ['Fake', 'True', 'Fake', 'True'],\n",
    "                'Type': ['Questions', 'Questions', 'Exclamations', 'Exclamations'],\n",
    "                'Percentage': [\n",
    "                    (df_fake['has_question'].sum() / len(df_fake) * 100),\n",
    "                    (df_true['has_question'].sum() / len(df_true) * 100),\n",
    "                    (df_fake['has_exclamation'].sum() / len(df_fake) * 100),\n",
    "                    (df_true['has_exclamation'].sum() / len(df_true) * 100)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            x = np.arange(2)\n",
    "            width = 0.35\n",
    "            axes[0, 1].bar(x - width/2, punct_data[punct_data['Category'] == 'Fake']['Percentage'], \n",
    "                          width, label='Fake', color='red', alpha=0.7)\n",
    "            axes[0, 1].bar(x + width/2, punct_data[punct_data['Category'] == 'True']['Percentage'], \n",
    "                          width, label='True', color='green', alpha=0.7)\n",
    "            axes[0, 1].set_ylabel('Percentage')\n",
    "            axes[0, 1].set_title('Punctuation Usage in Titles', fontweight='bold')\n",
    "            axes[0, 1].set_xticks(x)\n",
    "            axes[0, 1].set_xticklabels(['Questions (?)', 'Exclamations (!)'])\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # All caps words\n",
    "            axes[1, 0].hist(df_fake['all_caps_words'], bins=20, alpha=0.6, label='Fake', color='red')\n",
    "            axes[1, 0].hist(df_true['all_caps_words'], bins=20, alpha=0.6, label='True', color='green')\n",
    "            axes[1, 0].set_xlabel('Number of ALL CAPS Words')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].set_title('ALL CAPS Usage', fontweight='bold')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Title word count\n",
    "            axes[1, 1].hist(df_fake['title_word_count'], bins=20, alpha=0.6, label='Fake', color='red')\n",
    "            axes[1, 1].hist(df_true['title_word_count'], bins=20, alpha=0.6, label='True', color='green')\n",
    "            axes[1, 1].set_xlabel('Title Word Count')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].set_title('Title Word Count Distribution', fontweight='bold')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/title_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/title_analysis.png\")\n",
    "        \n",
    "        # ===== 1.3 WORD CLOUD ANALYSIS =====\n",
    "        if NLP_AVAILABLE and 'text' in df_text.columns:\n",
    "            print(\"\\nâ˜ï¸ Generating Word Clouds...\")\n",
    "            \n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "            # Fake news word cloud\n",
    "            fake_text = ' '.join(df_fake['text'].astype(str).tolist())\n",
    "            fake_text = re.sub(r'[^\\w\\s]', '', fake_text.lower())\n",
    "            \n",
    "            # True news word cloud\n",
    "            true_text = ' '.join(df_true['text'].astype(str).tolist())\n",
    "            true_text = re.sub(r'[^\\w\\s]', '', true_text.lower())\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            \n",
    "            wordcloud_fake = WordCloud(width=800, height=400, background_color='white', \n",
    "                                      stopwords=stop_words, colormap='Reds').generate(fake_text)\n",
    "            axes[0].imshow(wordcloud_fake, interpolation='bilinear')\n",
    "            axes[0].set_title('Fake News Word Cloud', fontweight='bold', fontsize=16)\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            wordcloud_true = WordCloud(width=800, height=400, background_color='white', \n",
    "                                      stopwords=stop_words, colormap='Greens').generate(true_text)\n",
    "            axes[1].imshow(wordcloud_true, interpolation='bilinear')\n",
    "            axes[1].set_title('True News Word Cloud', fontweight='bold', fontsize=16)\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/wordclouds.png\")\n",
    "        \n",
    "        # ===== 1.4 TOP N-GRAMS ANALYSIS =====\n",
    "        if 'text' in df_text.columns:\n",
    "            print(\"\\nðŸ“Š N-Gram Analysis...\")\n",
    "            \n",
    "            # Bigrams\n",
    "            vectorizer_bi = CountVectorizer(ngram_range=(2, 2), max_features=20, stop_words='english')\n",
    "            \n",
    "            fake_bigrams = vectorizer_bi.fit_transform(df_fake['text'].astype(str))\n",
    "            fake_bi_freq = dict(zip(vectorizer_bi.get_feature_names_out(), fake_bigrams.sum(axis=0).A1))\n",
    "            fake_bi_top = sorted(fake_bi_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            true_bigrams = vectorizer_bi.fit_transform(df_true['text'].astype(str))\n",
    "            true_bi_freq = dict(zip(vectorizer_bi.get_feature_names_out(), true_bigrams.sum(axis=0).A1))\n",
    "            true_bi_top = sorted(true_bi_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(\"\\nTop 10 Bigrams (Fake News):\")\n",
    "            for bigram, freq in fake_bi_top:\n",
    "                print(f\"  {bigram}: {freq:,}\")\n",
    "            \n",
    "            print(\"\\nTop 10 Bigrams (True News):\")\n",
    "            for bigram, freq in true_bi_top:\n",
    "                print(f\"  {bigram}: {freq:,}\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "            \n",
    "            fake_words, fake_counts = zip(*fake_bi_top)\n",
    "            axes[0].barh(range(len(fake_words)), fake_counts, color='red', alpha=0.7)\n",
    "            axes[0].set_yticks(range(len(fake_words)))\n",
    "            axes[0].set_yticklabels(fake_words)\n",
    "            axes[0].set_xlabel('Frequency')\n",
    "            axes[0].set_title('Top 10 Bigrams - Fake News', fontweight='bold', fontsize=14)\n",
    "            axes[0].invert_yaxis()\n",
    "            axes[0].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            true_words, true_counts = zip(*true_bi_top)\n",
    "            axes[1].barh(range(len(true_words)), true_counts, color='green', alpha=0.7)\n",
    "            axes[1].set_yticks(range(len(true_words)))\n",
    "            axes[1].set_yticklabels(true_words)\n",
    "            axes[1].set_xlabel('Frequency')\n",
    "            axes[1].set_title('Top 10 Bigrams - True News', fontweight='bold', fontsize=14)\n",
    "            axes[1].invert_yaxis()\n",
    "            axes[1].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/bigrams_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/bigrams_analysis.png\")\n",
    "        \n",
    "        # ===== 1.5 TF-IDF ANALYSIS =====\n",
    "        if 'text' in df_text.columns:\n",
    "            print(\"\\nðŸ” TF-IDF Analysis...\")\n",
    "            \n",
    "            tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "            tfidf_matrix = tfidf.fit_transform(df_text['text'].astype(str))\n",
    "            \n",
    "            print(f\"  TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "            print(f\"  Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "            \n",
    "            # PCA visualization\n",
    "            pca = PCA(n_components=2)\n",
    "            tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.scatter(tfidf_pca[df_text['label'] == 0, 0], \n",
    "                       tfidf_pca[df_text['label'] == 0, 1], \n",
    "                       c='red', alpha=0.3, s=20, label='Fake')\n",
    "            plt.scatter(tfidf_pca[df_text['label'] == 1, 0], \n",
    "                       tfidf_pca[df_text['label'] == 1, 1], \n",
    "                       c='green', alpha=0.3, s=20, label='True')\n",
    "            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "            plt.title('TF-IDF Feature Space (PCA)', fontweight='bold', fontsize=14)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.savefig('eda_outputs/tfidf_pca.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/tfidf_pca.png\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error in text analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# =====================================================\n",
    "# PHASE 2: ADVANCED IMAGE ANALYSIS\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PHASE 2] ADVANCED IMAGE DATASET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reddit_images = []\n",
    "df_reddit = None\n",
    "\n",
    "try:\n",
    "    if os.path.exists(REDDIT_PATH):\n",
    "        extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif']\n",
    "        for ext in extensions:\n",
    "            reddit_images.extend(glob.glob(os.path.join(REDDIT_PATH, '**', ext), recursive=True))\n",
    "        \n",
    "        print(f\"âœ“ Found {len(reddit_images)} Reddit images\")\n",
    "        \n",
    "        # Check for metadata\n",
    "        reddit_csv_files = glob.glob(os.path.join(REDDIT_PATH, '*.csv'))\n",
    "        if reddit_csv_files:\n",
    "            df_reddit = pd.read_csv(reddit_csv_files[0])\n",
    "            print(f\"âœ“ Reddit metadata: {len(df_reddit)} entries\")\n",
    "            \n",
    "        # ===== 2.1 IMAGE PROPERTIES ANALYSIS =====\n",
    "        if reddit_images and CV_AVAILABLE:\n",
    "            print(\"\\nðŸ“¸ Detailed Image Analysis...\")\n",
    "            sample_size = min(200, len(reddit_images))\n",
    "            sample_imgs = reddit_images[:sample_size]\n",
    "            \n",
    "            img_properties = {\n",
    "                'widths': [], 'heights': [], 'aspects': [], \n",
    "                'sizes': [], 'channels': [], 'formats': [],\n",
    "                'brightness': [], 'contrast': []\n",
    "            }\n",
    "            \n",
    "            for i, img_path in enumerate(sample_imgs):\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"  Processing {i}/{sample_size}...\", end='\\r')\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        h, w = img.shape[:2]\n",
    "                        img_properties['widths'].append(w)\n",
    "                        img_properties['heights'].append(h)\n",
    "                        img_properties['aspects'].append(w/h if h > 0 else 0)\n",
    "                        img_properties['sizes'].append(os.path.getsize(img_path)/1024)\n",
    "                        img_properties['channels'].append(img.shape[2] if len(img.shape) == 3 else 1)\n",
    "                        \n",
    "                        # Brightness and contrast\n",
    "                        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
    "                        img_properties['brightness'].append(np.mean(gray))\n",
    "                        img_properties['contrast'].append(np.std(gray))\n",
    "                        \n",
    "                        # Format\n",
    "                        ext = os.path.splitext(img_path)[1].lower()\n",
    "                        img_properties['formats'].append(ext)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\n\\nðŸ“Š Image Statistics (n={len(img_properties['widths'])}):\")\n",
    "            print(f\"  Dimensions: {np.mean(img_properties['widths']):.0f}x{np.mean(img_properties['heights']):.0f} (avg)\")\n",
    "            print(f\"  Aspect ratio: {np.mean(img_properties['aspects']):.2f} Â± {np.std(img_properties['aspects']):.2f}\")\n",
    "            print(f\"  File size: {np.mean(img_properties['sizes']):.1f} Â± {np.std(img_properties['sizes']):.1f} KB\")\n",
    "            print(f\"  Brightness: {np.mean(img_properties['brightness']):.1f} Â± {np.std(img_properties['brightness']):.1f}\")\n",
    "            print(f\"  Contrast: {np.mean(img_properties['contrast']):.1f} Â± {np.std(img_properties['contrast']):.1f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            \n",
    "            # Width distribution\n",
    "            axes[0, 0].hist(img_properties['widths'], bins=30, color='skyblue', edgecolor='black')\n",
    "            axes[0, 0].set_xlabel('Width (pixels)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].set_title('Image Width Distribution', fontweight='bold')\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Height distribution\n",
    "            axes[0, 1].hist(img_properties['heights'], bins=30, color='lightcoral', edgecolor='black')\n",
    "            axes[0, 1].set_xlabel('Height (pixels)')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].set_title('Image Height Distribution', fontweight='bold')\n",
    "            axes[0, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # Aspect ratio\n",
    "            axes[0, 2].hist(img_properties['aspects'], bins=30, color='lightgreen', edgecolor='black')\n",
    "            axes[0, 2].set_xlabel('Aspect Ratio (W/H)')\n",
    "            axes[0, 2].set_ylabel('Frequency')\n",
    "            axes[0, 2].set_title('Aspect Ratio Distribution', fontweight='bold')\n",
    "            axes[0, 2].grid(alpha=0.3)\n",
    "            \n",
    "            # File size\n",
    "            axes[1, 0].hist(img_properties['sizes'], bins=30, color='plum', edgecolor='black')\n",
    "            axes[1, 0].set_xlabel('File Size (KB)')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].set_title('File Size Distribution', fontweight='bold')\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Brightness\n",
    "            axes[1, 1].hist(img_properties['brightness'], bins=30, color='gold', edgecolor='black')\n",
    "            axes[1, 1].set_xlabel('Brightness')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].set_title('Brightness Distribution', fontweight='bold')\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # Contrast\n",
    "            axes[1, 2].hist(img_properties['contrast'], bins=30, color='orange', edgecolor='black')\n",
    "            axes[1, 2].set_xlabel('Contrast (Std Dev)')\n",
    "            axes[1, 2].set_ylabel('Frequency')\n",
    "            axes[1, 2].set_title('Contrast Distribution', fontweight='bold')\n",
    "            axes[1, 2].grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/image_properties.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/image_properties.png\")\n",
    "            \n",
    "            # Format distribution\n",
    "            format_counts = Counter(img_properties['formats'])\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            formats, counts = zip(*format_counts.most_common())\n",
    "            plt.bar(formats, counts, color='teal', alpha=0.7, edgecolor='black')\n",
    "            plt.xlabel('Image Format')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Image Format Distribution', fontweight='bold', fontsize=14)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.savefig('eda_outputs/image_formats.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/image_formats.png\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error in image analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# =====================================================\n",
    "# PHASE 3: CNN DEEP FEATURE ANALYSIS\n",
    "# =====================================================\n",
    "if DL_AVAILABLE and reddit_images:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[PHASE 3] DEEP CNN FEATURE EXTRACTION & ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nðŸ§  Loading pre-trained models...\")\n",
    "        resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        def extract_deep_features(img_path, model):\n",
    "            try:\n",
    "                img = load_img(img_path, target_size=(224, 224))\n",
    "                img_array = img_to_array(img)\n",
    "                img_array = np.expand_dims(img_array, axis=0)\n",
    "                img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "                features = model.predict(img_array, verbose=0)\n",
    "                return features.flatten()\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        sample_images = reddit_images[:min(100, len(reddit_images))]\n",
    "        print(f\"ðŸ” Extracting features from {len(sample_images)} images...\")\n",
    "        \n",
    "        features_list = []\n",
    "        valid_images = []\n",
    "        \n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Progress: {i}/{len(sample_images)}\", end='\\r')\n",
    "            feat = extract_deep_features(img_path, resnet_model)\n",
    "            if feat is not None:\n",
    "                features_list.append(feat)\n",
    "                valid_images.append(img_path)\n",
    "        \n",
    "        if features_list:\n",
    "            features_array = np.array(features_list)\n",
    "            print(f\"\\nâœ“ Extracted features shape: {features_array.shape}\")\n",
    "            \n",
    "            # Feature statistics\n",
    "            print(f\"\\nðŸ“ˆ Deep Feature Statistics:\")\n",
    "            print(f\"  Mean activation: {features_array.mean():.4f}\")\n",
    "            print(f\"  Std activation: {features_array.std():.4f}\")\n",
    "            print(f\"  Max activation: {features_array.max():.4f}\")\n",
    "            print(f\"  Min activation: {features_array.min():.4f}\")\n",
    "            print(f\"  Sparsity: {(features_array == 0).sum() / features_array.size * 100:.2f}%\")\n",
    "            \n",
    "            # Activation distribution\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(features_array.flatten(), bins=100, color='purple', alpha=0.7, edgecolor='black')\n",
    "            plt.xlabel('Activation Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('CNN Feature Activation Distribution', fontweight='bold', fontsize=12)\n",
    "            plt.yscale('log')\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            # Feature variance\n",
    "            feature_vars = np.var(features_array, axis=0)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(feature_vars, color='darkblue', linewidth=1)\n",
    "            plt.xlabel('Feature Index')\n",
    "            plt.ylabel('Variance')\n",
    "            plt.title('Feature Variance Across Images', fontweight='bold', fontsize=12)\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/cnn_feature_distribution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/cnn_feature_distribution.png\")\n",
    "            \n",
    "            # t-SNE visualization\n",
    "            try:\n",
    "                from sklearn.manifold import TSNE\n",
    "                print(\"\\nðŸ“Š Creating t-SNE visualization...\")\n",
    "                \n",
    "                n_samples = min(100, len(features_array))\n",
    "                perplexity_val = min(30, n_samples - 1)\n",
    "                \n",
    "                if n_samples > 5 and perplexity_val > 1:\n",
    "                    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val, n_iter=1000)\n",
    "                    features_2d = tsne.fit_transform(features_array[:n_samples])\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 10))\n",
    "                    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                                        c=range(n_samples), cmap='viridis', \n",
    "                                        s=100, alpha=0.6, edgecolors='black')\n",
    "                    plt.colorbar(scatter, label='Image Index')\n",
    "                    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "                    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "                    plt.title('Image Feature Space Visualization (t-SNE)', fontweight='bold', fontsize=14)\n",
    "                    plt.grid(alpha=0.3)\n",
    "                    plt.savefig('eda_outputs/tsne_features.png', dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(\"âœ“ Saved: eda_outputs/tsne_features.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  t-SNE visualization failed: {e}\")\n",
    "            \n",
    "            # UMAP visualization (if available)\n",
    "            try:\n",
    "                from umap import UMAP\n",
    "                print(\"\\nðŸ“Š Creating UMAP visualization...\")\n",
    "                \n",
    "                umap_model = UMAP(n_components=2, random_state=42, n_neighbors=15)\n",
    "                features_umap = umap_model.fit_transform(features_array[:min(100, len(features_array))])\n",
    "                \n",
    "                plt.figure(figsize=(12, 10))\n",
    "                scatter = plt.scatter(features_umap[:, 0], features_umap[:, 1], \n",
    "                                    c=range(len(features_umap)), cmap='plasma', \n",
    "                                    s=100, alpha=0.6, edgecolors='black')\n",
    "                plt.colorbar(scatter, label='Image Index')\n",
    "                plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "                plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "                plt.title('Image Feature Space Visualization (UMAP)', fontweight='bold', fontsize=14)\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.savefig('eda_outputs/umap_features.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(\"âœ“ Saved: eda_outputs/umap_features.png\")\n",
    "            except:\n",
    "                print(\"âš  UMAP not available (install: pip install umap-learn)\")\n",
    "            \n",
    "            # PCA analysis\n",
    "            print(\"\\nðŸ“Š PCA Analysis...\")\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA()\n",
    "            pca.fit(features_array)\n",
    "            \n",
    "            # Explained variance\n",
    "            cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "            n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "            n_components_99 = np.argmax(cumsum_var >= 0.99) + 1\n",
    "            \n",
    "            print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "            print(f\"  Components for 99% variance: {n_components_99}\")\n",
    "            \n",
    "            plt.figure(figsize=(14, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(range(1, min(51, len(pca.explained_variance_ratio_)+1)), \n",
    "                    pca.explained_variance_ratio_[:50], 'b-', linewidth=2)\n",
    "            plt.xlabel('Principal Component')\n",
    "            plt.ylabel('Explained Variance Ratio')\n",
    "            plt.title('PCA - Explained Variance per Component', fontweight='bold')\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(1, min(101, len(cumsum_var)+1)), cumsum_var[:100], 'r-', linewidth=2)\n",
    "            plt.axhline(y=0.95, color='g', linestyle='--', label='95% variance')\n",
    "            plt.axhline(y=0.99, color='b', linestyle='--', label='99% variance')\n",
    "            plt.xlabel('Number of Components')\n",
    "            plt.ylabel('Cumulative Explained Variance')\n",
    "            plt.title('PCA - Cumulative Explained Variance', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/pca_analysis.png\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error in CNN analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =====================================================\n",
    "# PHASE 4: CORRELATION & STATISTICAL ANALYSIS\n",
    "# =====================================================\n",
    "if df_text is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[PHASE 4] CORRELATION & STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Create numerical features for correlation\n",
    "        print(\"\\nðŸ“Š Computing correlations...\")\n",
    "        \n",
    "        numerical_features = df_text.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if len(numerical_features) > 1:\n",
    "            corr_matrix = df_text[numerical_features].corr()\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                       center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "            plt.title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('eda_outputs/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ“ Saved: eda_outputs/correlation_matrix.png\")\n",
    "        \n",
    "        # Statistical tests\n",
    "        from scipy import stats\n",
    "        \n",
    "        print(\"\\nðŸ”¬ Statistical Tests (Fake vs True):\")\n",
    "        \n",
    "        if 'text_length' in df_text.columns:\n",
    "            # T-test for text length\n",
    "            fake_lengths = df_fake['text_length']\n",
    "            true_lengths = df_true['text_length']\n",
    "            t_stat, p_value = stats.ttest_ind(fake_lengths, true_lengths)\n",
    "            print(f\"\\n  Text Length T-Test:\")\n",
    "            print(f\"    t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"    p-value: {p_value:.6f}\")\n",
    "            print(f\"    Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        if 'word_count' in df_text.columns:\n",
    "            # Mann-Whitney U test for word count\n",
    "            fake_words = df_fake['word_count']\n",
    "            true_words = df_true['word_count']\n",
    "            u_stat, p_value = stats.mannwhitneyu(fake_words, true_words)\n",
    "            print(f\"\\n  Word Count Mann-Whitney U Test:\")\n",
    "            print(f\"    U-statistic: {u_stat:.4f}\")\n",
    "            print(f\"    p-value: {p_value:.6f}\")\n",
    "            print(f\"    Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        if 'title_length' in df_text.columns:\n",
    "            # Chi-square test for categorical features\n",
    "            contingency_table = pd.crosstab(df_text['has_exclamation'], df_text['label'])\n",
    "            chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "            print(f\"\\n  Exclamation vs Label Chi-Square Test:\")\n",
    "            print(f\"    chi2-statistic: {chi2:.4f}\")\n",
    "            print(f\"    p-value: {p_value:.6f}\")\n",
    "            print(f\"    Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error in statistical analysis: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# PHASE 5: COMPREHENSIVE SUMMARY DASHBOARD\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[PHASE 5] GENERATING COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Dataset sizes\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    if df_text is not None:\n",
    "        sizes = [len(df_fake), len(df_true), len(reddit_images)]\n",
    "        labels = ['Fake Text', 'True Text', 'Images']\n",
    "        colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "        ax1.bar(labels, sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.set_title('Dataset Distribution', fontweight='bold')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        for i, v in enumerate(sizes):\n",
    "            ax1.text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Text length comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    if df_text is not None and 'text_length' in df_text.columns:\n",
    "        data = [df_fake['text_length'], df_true['text_length']]\n",
    "        bp = ax2.boxplot(data, labels=['Fake', 'True'], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('#e74c3c')\n",
    "        bp['boxes'][1].set_facecolor('#2ecc71')\n",
    "        ax2.set_ylabel('Characters')\n",
    "        ax2.set_title('Text Length Distribution', fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Word count comparison\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    if df_text is not None and 'word_count' in df_text.columns:\n",
    "        data = [df_fake['word_count'], df_true['word_count']]\n",
    "        bp = ax3.boxplot(data, labels=['Fake', 'True'], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('#e74c3c')\n",
    "        bp['boxes'][1].set_facecolor('#2ecc71')\n",
    "        ax3.set_ylabel('Words')\n",
    "        ax3.set_title('Word Count Distribution', fontweight='bold')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Title characteristics\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    if df_text is not None and 'has_question' in df_text.columns:\n",
    "        fake_q = (df_fake['has_question'].sum() / len(df_fake) * 100)\n",
    "        true_q = (df_true['has_question'].sum() / len(df_true) * 100)\n",
    "        fake_e = (df_fake['has_exclamation'].sum() / len(df_fake) * 100)\n",
    "        true_e = (df_true['has_exclamation'].sum() / len(df_true) * 100)\n",
    "        \n",
    "        x = np.arange(2)\n",
    "        width = 0.35\n",
    "        ax4.bar(x - width/2, [fake_q, fake_e], width, label='Fake', color='#e74c3c', alpha=0.7)\n",
    "        ax4.bar(x + width/2, [true_q, true_e], width, label='True', color='#2ecc71', alpha=0.7)\n",
    "        ax4.set_ylabel('Percentage (%)')\n",
    "        ax4.set_title('Punctuation in Titles', fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(['Questions', 'Exclamations'])\n",
    "        ax4.legend()\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Image properties (if available)\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    if reddit_images and CV_AVAILABLE:\n",
    "        sample_imgs = reddit_images[:50]\n",
    "        sizes = []\n",
    "        for img_path in sample_imgs:\n",
    "            try:\n",
    "                sizes.append(os.path.getsize(img_path)/1024)\n",
    "            except:\n",
    "                continue\n",
    "        if sizes:\n",
    "            ax5.hist(sizes, bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "            ax5.set_xlabel('File Size (KB)')\n",
    "            ax5.set_ylabel('Frequency')\n",
    "            ax5.set_title('Image File Sizes', fontweight='bold')\n",
    "            ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Data quality metrics\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    if df_text is not None:\n",
    "        metrics = {\n",
    "            'Text Samples': len(df_text),\n",
    "            'Image Samples': len(reddit_images),\n",
    "            'Missing Values': df_text.isnull().sum().sum(),\n",
    "            'Duplicates': df_text.duplicated().sum()\n",
    "        }\n",
    "        ax6.barh(list(metrics.keys()), list(metrics.values()), color='#9b59b6', alpha=0.7)\n",
    "        ax6.set_xlabel('Count')\n",
    "        ax6.set_title('Data Quality Metrics', fontweight='bold')\n",
    "        ax6.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 7. Class balance\n",
    "    ax7 = fig.add_subplot(gs[2, 0])\n",
    "    if df_text is not None:\n",
    "        class_counts = df_text['label'].value_counts()\n",
    "        colors = ['#e74c3c', '#2ecc71']\n",
    "        ax7.pie(class_counts.values, labels=['Fake', 'True'], autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "        ax7.set_title('Class Balance', fontweight='bold')\n",
    "    \n",
    "    # 8. Summary statistics table\n",
    "    ax8 = fig.add_subplot(gs[2, 1:])\n",
    "    ax8.axis('tight')\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    if df_text is not None:\n",
    "        summary_data = []\n",
    "        summary_data.append(['Total Articles', f\"{len(df_text):,}\"])\n",
    "        summary_data.append(['Fake Articles', f\"{len(df_fake):,}\"])\n",
    "        summary_data.append(['True Articles', f\"{len(df_true):,}\"])\n",
    "        summary_data.append(['Total Images', f\"{len(reddit_images):,}\"])\n",
    "        if 'text_length' in df_text.columns:\n",
    "            summary_data.append(['Avg Text Length (Fake)', f\"{df_fake['text_length'].mean():.0f} chars\"])\n",
    "            summary_data.append(['Avg Text Length (True)', f\"{df_true['text_length'].mean():.0f} chars\"])\n",
    "        \n",
    "        table = ax8.table(cellText=summary_data, colLabels=['Metric', 'Value'],\n",
    "                         cellLoc='left', loc='center',\n",
    "                         colWidths=[0.5, 0.5])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        for i in range(len(summary_data) + 1):\n",
    "            if i == 0:\n",
    "                table[(i, 0)].set_facecolor('#34495e')\n",
    "                table[(i, 1)].set_facecolor('#34495e')\n",
    "                table[(i, 0)].set_text_props(weight='bold', color='white')\n",
    "                table[(i, 1)].set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                table[(i, 0)].set_facecolor('#ecf0f1')\n",
    "                table[(i, 1)].set_facecolor('#ecf0f1')\n",
    "    \n",
    "    plt.suptitle('Comprehensive EDA Summary Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.savefig('eda_outputs/summary_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ“ Saved: eda_outputs/summary_dashboard.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error creating summary dashboard: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# =====================================================\n",
    "# FINAL SUMMARY\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ADVANCED EDA COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ Generated Outputs (in eda_outputs/):\")\n",
    "output_files = [\n",
    "    'text_length_analysis.png',\n",
    "    'title_analysis.png',\n",
    "    'wordclouds.png',\n",
    "    'bigrams_analysis.png',\n",
    "    'tfidf_pca.png',\n",
    "    'image_properties.png',\n",
    "    'image_formats.png',\n",
    "    'cnn_feature_distribution.png',\n",
    "    'tsne_features.png',\n",
    "    'pca_analysis.png',\n",
    "    'correlation_matrix.png',\n",
    "    'summary_dashboard.png'\n",
    "]\n",
    "\n",
    "for filename in output_files:\n",
    "    filepath = os.path.join('eda_outputs', filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  âœ“ {filename}\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Analysis Components Completed:\")\n",
    "print(\"  âœ“ Text length & word count analysis\")\n",
    "print(\"  âœ“ Title characteristics (punctuation, caps)\")\n",
    "print(\"  âœ“ Word clouds (fake vs true)\")\n",
    "print(\"  âœ“ N-gram analysis (bigrams)\")\n",
    "print(\"  âœ“ TF-IDF feature extraction & PCA\")\n",
    "print(\"  âœ“ Image properties (dimensions, brightness, contrast)\")\n",
    "print(\"  âœ“ Deep CNN feature extraction (ResNet50)\")\n",
    "print(\"  âœ“ t-SNE & UMAP visualizations\")\n",
    "print(\"  âœ“ PCA dimensionality analysis\")\n",
    "print(\"  âœ“ Statistical hypothesis testing\")\n",
    "print(\"  âœ“ Correlation analysis\")\n",
    "print(\"  âœ“ Comprehensive summary dashboard\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insights:\")\n",
    "if df_text is not None:\n",
    "    print(f\"  â€¢ Dataset balance: {len(df_fake):,} fake vs {len(df_true):,} true articles\")\n",
    "    if 'text_length' in df_text.columns:\n",
    "        fake_avg = df_fake['text_length'].mean()\n",
    "        true_avg = df_true['text_length'].mean()\n",
    "        diff_pct = abs(fake_avg - true_avg) / true_avg * 100\n",
    "        print(f\"  â€¢ Text length difference: {diff_pct:.1f}%\")\n",
    "    if 'has_exclamation' in df_text.columns:\n",
    "        fake_exc = (df_fake['has_exclamation'].sum() / len(df_fake) * 100)\n",
    "        true_exc = (df_true['has_exclamation'].sum() / len(df_true) * 100)\n",
    "        print(f\"  â€¢ Exclamation usage: {fake_exc:.1f}% (fake) vs {true_exc:.1f}% (true)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"  1. Feature engineering based on EDA insights\")\n",
    "print(\"  2. Model selection (CNN for images, LSTM/Transformer for text)\")\n",
    "print(\"  3. Multimodal fusion architecture design\")\n",
    "print(\"  4. Training with proper validation strategy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94c001-f1c7-4264-abf9-2111026daf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
